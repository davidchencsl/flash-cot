from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def batch_inference(model_id: str, prompts: list):
    """
    Perform batch inference with GPU acceleration, ensuring the prompt tokens are skipped.
    
    Args:
        model_id (str): Identifier for the model (Hugging Face model hub name).
        prompts (list): List of prompts (strings) for inference.
        
    Returns:
        list: Responses generated by the model for each prompt.
    """

    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

    tokenizer.pad_token_id = tokenizer.eos_token_id

    # Tokenize prompts for batch processing
    inputs = tokenizer(prompts, padding="longest", return_tensors="pt")
    inputs = {key: val.to("cuda") for key, val in inputs.items()}  # Move inputs to GPU

    # Generate responses
    try:
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
        )

        # Decode responses and remove prompt text
        responses = []
        for i, output in enumerate(outputs):
            # Decode the output tokens
            decoded = tokenizer.decode(output, skip_special_tokens=True)

            # Remove the prompt text from the start of the response
            response = decoded[len(prompts[i]):].strip()
            responses.append(response)

        return responses
    except Exception as e:
        print(f"Error during inference: {e}")
        return []
    
    