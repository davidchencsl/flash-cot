from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def batch_inference(model_id: str, prompts: list):
    """
    Perform batch inference with GPU acceleration, ensuring the prompt tokens are skipped.
    
    Args:
        model_id (str): Identifier for the model (Hugging Face model hub name).
        prompts (list): List of prompts (strings) for inference.
        
    Returns:
        list: Responses generated by the model for each prompt.
    """

    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_id, device_map="auto")
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

    tokenizer.pad_token_id = tokenizer.eos_token_id

    # Tokenize prompts for batch processing
    inputs = tokenizer(prompts, padding="longest", return_tensors="pt")
    inputs = {key: val.to("cuda") for key, val in inputs.items()}  # Move inputs to GPU

    # Generate responses
    try:
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
        )

        responses = []
        # Decode responses and remove prompt text
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        for i, raw_response in enumerate(decoded):
            # Remove the prompt text from the start of the response
            response = raw_response[len(prompts[i]):].strip()
            responses.append(response)
            # print(f"Prompt: {prompts[i]}")
            # print(f"Raw: {raw_response}")
            # print(f"Response: {response}")
            # print(f"Prompt length: {len(prompts[i])}")

        return responses
    except Exception as e:
        print(f"Error during inference: {e}")
        return []
    

def batch_flash_cot(prompts: list):
    cot_responses = batch_inference("meta-llama/Llama-3.1-8B-Instruct", prompts+"\nThink through the problem step by step.")
    full_prompts = [a+"\n"+b for a, b in zip(prompts, cot_responses)]
    responses = batch_inference("fsaudm/Meta-Llama-3.1-70B-Instruct-INT8", full_prompts+'\nUse the information provided to answer the question.')
    return responses
    